{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianomunga/backTrack/blob/main/rails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzRMqwmRvIMe",
        "outputId": "83dac483-9857-467c-908d-837b4376539d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets_df = pd.read_csv('/content/health_tweets.csv')\n",
        "tweets_df.dropna(axis='columns', inplace=True)\n",
        "tweets_df.columns\n",
        "tweets_df = tweets_df[['date', 'timezone', 'tweet', \n",
        "                       'hashtags', 'username', 'name', \n",
        "                       'day', 'hour', 'retweet', 'nlikes', \n",
        "                       'nreplies', 'nretweets']]\n",
        "tweets_df.drop_duplicates(inplace=True, subset=\"tweet\")\n",
        "\n",
        "import re\n",
        "import gensim\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'       \n",
        "# define a string of punctuation symbols\n",
        "\n",
        "# Functions to clean tweets\n",
        "def remove_links(tweet):\n",
        "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)   # remove http links\n",
        "    tweet = re.sub(r'bit.ly/\\S+', '', tweet)  # remove bitly links\n",
        "    tweet = tweet.strip('[link]')   # remove [links]\n",
        "    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n",
        "    return tweet\n",
        "\n",
        "def remove_users(tweet):\n",
        "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
        "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n",
        "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at\n",
        "    return tweet\n",
        "\n",
        "def remove_hashtags(tweet):\n",
        "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
        "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
        "    return tweet\n",
        "\n",
        "def remove_av(tweet):\n",
        "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
        "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
        "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
        "    return tweet\n",
        "\n",
        "def tokenize(tweet):\n",
        "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(tweet):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
        "                and len(token) > 2:  # drops words with less than 3 characters\n",
        "            result.append(lemmatize(token))\n",
        "    return result\n",
        "\n",
        "def lemmatize(token):\n",
        "    \"\"\"Returns lemmatization of a token\"\"\"\n",
        "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
        "    tweet = remove_users(tweet)\n",
        "    tweet = remove_links(tweet)\n",
        "    tweet = remove_hashtags(tweet)\n",
        "    tweet = remove_av(tweet)\n",
        "    tweet = tweet.lower()  # lower case\n",
        "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
        "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
        "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
        "    tweet_token_list = tokenize(tweet)  # apply lemmatization and tokenization\n",
        "    tweet = ' '.join(tweet_token_list)\n",
        "    return tweet\n",
        "\n",
        "def basic_clean(tweet):\n",
        "    \"\"\"Main master function to clean tweets only without tokenization or removal of stopwords\"\"\"\n",
        "    tweet = remove_users(tweet)\n",
        "    tweet = remove_links(tweet)\n",
        "    tweet = remove_hashtags(tweet)\n",
        "    tweet = remove_av(tweet)\n",
        "    tweet = tweet.lower()  # lower case\n",
        "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
        "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
        "    tweet = re.sub('([0-9]+)', '', tweet)  # remove numbers\n",
        "    tweet = re.sub('üìù ‚Ä¶', '', tweet)\n",
        "    return tweet\n",
        "\n",
        "def tokenize_tweets(df):\n",
        "    \"\"\"Main function to read in and return cleaned and preprocessed dataframe.\n",
        "    This can be used in Jupyter notebooks by importing this module and calling the tokenize_tweets() function\n",
        "    Args:\n",
        "        df = data frame object to apply cleaning to\n",
        "    Returns:\n",
        "        pandas data frame with cleaned tokens\n",
        "    \"\"\"\n",
        "\n",
        "    df['tokens'] = df.tweet.apply(preprocess_tweet)\n",
        "    num_tweets = len(df)\n",
        "    print('Complete. Number of Tweets that have been cleaned and tokenized : {}'.format(num_tweets))\n",
        "    return df\n",
        "\n",
        "tweets_df = tokenize_tweets(tweets_df)\n",
        "tweets_df.head(69)"
      ],
      "metadata": {
        "id": "Nqpp_WRQjSmn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "a31b30ca-d0b4-4723-acc7-c6f644681161"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Complete. Number of Tweets that have been cleaned and tokenized : 197925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>tweet</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-28 00:10:53</td>\n",
              "      <td>UTC</td>\n",
              "      <td>How the UK‚Äôs coronavirus epidemic compares to ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>False</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>coronavirus epidemic compare countries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-27 17:46:51</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Health workers on frontline to be tested in En...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>False</td>\n",
              "      <td>34</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>health workers frontline test england</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-27 06:02:12</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Coronavirus: Protective gear guidance 'to be u...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>False</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>coronavirus protective gear guidance update</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-27 02:03:35</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Coronavirus: What are ventilators and why are ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>False</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>coronavirus ventilators important</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-26 19:58:34</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Coronavirus: 'Act early to save more than 30 m...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>coronavirus act early save million live</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>2020-02-19 02:26:05</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Children facing uncertain future, experts warn...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>False</td>\n",
              "      <td>21</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>children face uncertain future experts warn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>2020-02-18 17:14:12</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Heart doctors 'held back stent death data' htt...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>33</td>\n",
              "      <td>heart doctor hold stent death data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>2020-02-18 00:59:12</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Criminals end up with 'smaller brains' http://...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>criminals end smaller brain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2020-02-18 00:03:55</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Transgender patients self-medicating over NHS ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>False</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>transgender patients self medicate nhs wait</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>2020-02-17 17:58:37</td>\n",
              "      <td>UTC</td>\n",
              "      <td>Gene therapy to halt rare form of sight loss h...</td>\n",
              "      <td>[]</td>\n",
              "      <td>bbchealth</td>\n",
              "      <td>BBC Health News</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>gene therapy halt rare form sight loss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>69 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   date  ...                                       tokens\n",
              "0   2020-03-28 00:10:53  ...       coronavirus epidemic compare countries\n",
              "1   2020-03-27 17:46:51  ...        health workers frontline test england\n",
              "2   2020-03-27 06:02:12  ...  coronavirus protective gear guidance update\n",
              "3   2020-03-27 02:03:35  ...            coronavirus ventilators important\n",
              "4   2020-03-26 19:58:34  ...      coronavirus act early save million live\n",
              "..                  ...  ...                                          ...\n",
              "64  2020-02-19 02:26:05  ...  children face uncertain future experts warn\n",
              "65  2020-02-18 17:14:12  ...           heart doctor hold stent death data\n",
              "66  2020-02-18 00:59:12  ...                  criminals end smaller brain\n",
              "67  2020-02-18 00:03:55  ...  transgender patients self medicate nhs wait\n",
              "68  2020-02-17 17:58:37  ...       gene therapy halt rare form sight loss\n",
              "\n",
              "[69 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "rails.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKGJfDxFgM9gnEhngasYtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}